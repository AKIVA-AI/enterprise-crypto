"""
CUDA Engine - GPU-Accelerated Computing Core

Provides hardware abstraction for GPU operations with automatic fallback.
Supports NVIDIA CUDA, Apple Metal, and CPU backends.
"""

import logging
from typing import Optional, Dict, Any, List
from enum import Enum
from dataclasses import dataclass, field
import numpy as np

logger = logging.getLogger(__name__)


class GPUBackend(Enum):
    """Available GPU backends."""
    CUDA = "cuda"
    METAL = "metal"
    CPU = "cpu"


@dataclass
class GPUDeviceInfo:
    """GPU device information."""
    name: str
    memory_total: int  # bytes
    memory_free: int  # bytes
    compute_capability: str
    backend: GPUBackend
    device_id: int = 0


@dataclass
class GPUConfig:
    """GPU configuration settings."""
    preferred_backend: GPUBackend = GPUBackend.CUDA
    memory_fraction: float = 0.8  # Use 80% of GPU memory
    enable_tensor_cores: bool = True
    enable_mixed_precision: bool = True
    batch_size: int = 1024
    num_workers: int = 4


class GPUEngine:
    """
    Enterprise-grade GPU acceleration engine.
    
    Features:
    - Automatic backend detection (CUDA > Metal > CPU)
    - Memory management and optimization
    - Tensor operations
    - Mixed precision support
    - Multi-GPU support
    """
    
    _instance: Optional['GPUEngine'] = None
    
    def __init__(self, config: Optional[GPUConfig] = None):
        self.config = config or GPUConfig()
        self._backend: Optional[GPUBackend] = None
        self._device_info: Optional[GPUDeviceInfo] = None
        self._initialized = False
        
        # Lazy imports for GPU libraries
        self._torch = None
        self._cupy = None
        self._cuml = None
        self._cudf = None
    
    @classmethod
    def get_instance(cls, config: Optional[GPUConfig] = None) -> 'GPUEngine':
        """Get singleton instance."""
        if cls._instance is None:
            cls._instance = cls(config)
        return cls._instance
    
    def initialize(self) -> bool:
        """Initialize GPU engine with best available backend."""
        if self._initialized:
            return True
        
        # Try CUDA first
        if self._try_cuda():
            self._backend = GPUBackend.CUDA
            logger.info(f"GPU Engine initialized with CUDA: {self._device_info.name}")
            self._initialized = True
            return True
        
        # Try Metal (macOS)
        if self._try_metal():
            self._backend = GPUBackend.METAL
            logger.info(f"GPU Engine initialized with Metal: {self._device_info.name}")
            self._initialized = True
            return True
        
        # Fallback to CPU
        self._backend = GPUBackend.CPU
        self._device_info = GPUDeviceInfo(
            name="CPU",
            memory_total=0,
            memory_free=0,
            compute_capability="N/A",
            backend=GPUBackend.CPU
        )
        logger.warning("GPU not available, using CPU backend")
        self._initialized = True
        return True
    
    def _try_cuda(self) -> bool:
        """Try to initialize CUDA backend."""
        try:
            import torch
            if torch.cuda.is_available():
                self._torch = torch
                device = torch.cuda.current_device()
                props = torch.cuda.get_device_properties(device)
                
                self._device_info = GPUDeviceInfo(
                    name=props.name,
                    memory_total=props.total_memory,
                    memory_free=torch.cuda.mem_get_info()[0],
                    compute_capability=f"{props.major}.{props.minor}",
                    backend=GPUBackend.CUDA,
                    device_id=device
                )
                
                # Try to import RAPIDS
                try:
                    import cupy as cp
                    import cuml
                    import cudf
                    self._cupy = cp
                    self._cuml = cuml
                    self._cudf = cudf
                    logger.info("RAPIDS libraries loaded (cuML, cuDF)")
                except ImportError:
                    logger.info("RAPIDS not available, using PyTorch only")
                
                return True
        except Exception as e:
            logger.debug(f"CUDA not available: {e}")
        return False
    
    def _try_metal(self) -> bool:
        """Try to initialize Metal backend (macOS)."""
        try:
            import torch
            if torch.backends.mps.is_available():
                self._torch = torch
                self._device_info = GPUDeviceInfo(
                    name="Apple Metal",
                    memory_total=0,  # MPS doesn't report this
                    memory_free=0,
                    compute_capability="Metal",
                    backend=GPUBackend.METAL
                )
                return True
        except Exception as e:
            logger.debug(f"Metal not available: {e}")
        return False
    
    @property
    def backend(self) -> GPUBackend:
        """Get current backend."""
        if not self._initialized:
            self.initialize()
        return self._backend
    
    @property
    def device_info(self) -> Optional[GPUDeviceInfo]:
        """Get device information."""
        if not self._initialized:
            self.initialize()
        return self._device_info
    
    @property
    def is_gpu_available(self) -> bool:
        """Check if GPU is available."""
        return self.backend in (GPUBackend.CUDA, GPUBackend.METAL)
    
    def get_device(self) -> str:
        """Get PyTorch device string."""
        if self.backend == GPUBackend.CUDA:
            return f"cuda:{self._device_info.device_id}"
        elif self.backend == GPUBackend.METAL:
            return "mps"
        return "cpu"
    
    def to_tensor(self, data: np.ndarray) -> Any:
        """Convert numpy array to GPU tensor."""
        if not self._initialized:
            self.initialize()
        
        if self._torch is not None:
            tensor = self._torch.from_numpy(data)
            return tensor.to(self.get_device())
        return data
    
    def get_status(self) -> Dict[str, Any]:
        """Get GPU engine status."""
        if not self._initialized:
            self.initialize()
        
        status = {
            "initialized": self._initialized,
            "backend": self._backend.value if self._backend else None,
            "gpu_available": self.is_gpu_available,
        }
        
        if self._device_info:
            status["device"] = {
                "name": self._device_info.name,
                "memory_total_gb": self._device_info.memory_total / (1024**3),
                "memory_free_gb": self._device_info.memory_free / (1024**3),
                "compute_capability": self._device_info.compute_capability,
            }
        
        return status


# Global instance getter
def get_gpu_engine(config: Optional[GPUConfig] = None) -> GPUEngine:
    """Get GPU engine singleton."""
    return GPUEngine.get_instance(config)

